{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34cc8ac-e2bb-4d75-b31f-c3da9824841d",
   "metadata": {},
   "source": [
    "# MiniGPT: A Lightweight Transformer-based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d184fe78-eddb-4c5e-a92f-64324f203666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae685c-8cd4-465f-a926-fe694f1299e4",
   "metadata": {},
   "source": [
    "## Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b05f87-476d-4427-85b7-7bee7df6a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add a padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258b81e-f89e-49bd-beb6-aa7de0f26a6a",
   "metadata": {},
   "source": [
    "## Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cea799-2c65-45ab-9a31-d6e671e9cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention Mechanism\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = embed_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Softmax for probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2defaa-0872-47df-8678-1e0e288d966d",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0676fa18-fbdb-4269-8b85-b1f5fb5208f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-Attention\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # Feed-Forward\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d651b-1ce9-4bc1-9576-18b82f99b629",
   "metadata": {},
   "source": [
    "## Mini GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d414de3-4703-4ab9-91aa-938f76484215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, ff_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, ff_dim, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input embedding\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Transformer layers\n",
    "        for layer in self.transformers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Output layer\n",
    "        logits = self.output(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f49fe0-f530-44ea-8618-49e5537d3b6d",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a6a64f-8318-48ea-881b-f223ed67ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and tokenize\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# DataLoader-compatible dataset\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed41e8d-1821-45f0-9839-52c4da9feac8",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783efcd4-0998-4929-a5a9-06d285151e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer setup\n",
    "vocab_size = len(tokenizer)\n",
    "model = MiniGPT(vocab_size, embed_dim=1024, num_layers=8, ff_dim=2048, num_heads=8)\n",
    "\n",
    "# Adjust token embeddings if the tokenizer size was updated\n",
    "model.embedding = nn.Embedding(vocab_size, model.embedding.embedding_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Example: 5 epochs\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Shift labels for causal language modeling\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769c7cf-1cf8-4f2c-9a00-2f95247768a2",
   "metadata": {},
   "source": [
    "## Text Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a7e30-411b-418e-83cd-17102e258994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_p=0.9):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)[:, -1, :]  # Get logits for the last token\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Apply nucleus (top-p) sampling\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            logits[sorted_indices[sorted_indices_to_remove]] = -float('Inf')\n",
    "            \n",
    "            # Sample from the filtered distribution\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        \n",
    "        # Append the generated token to the sequence\n",
    "        input_ids = torch.cat((input_ids, next_token), dim=1)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacdd44-8384-4b3e-b8f9-d9648c15e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print text\n",
    "prompt = \"Machine learning is a subset of artificial intelligence that\"\n",
    "output = generate_text(model, tokenizer, prompt, temperature=0.8, top_p=0.9)\n",
    "print(\"Generated Text:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75018f-aaa2-43ba-ad1b-69890ca4b606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
